import logging
import os
import io
from typing import List, Dict, Optional, Iterator

import torch
import ollama
from PyPDF2 import PdfReader
from openai import OpenAI


VAULT_PATH = os.environ.get("VAULT_PATH", "vault.txt")
# DEFAULT_MODEL = os.environ.get("OLLAMA_MODEL", "deepseek-r1:7b")
DEFAULT_MODEL = os.environ.get("OLLAMA_MODEL", "llama3:latest")  # ä½¿ç”¨æ›´å°çš„æ¨¡å‹


def read_vault_lines(vault_path: str) -> List[str]:
    if not os.path.exists(vault_path):
        return []
    with open(vault_path, 'r', encoding='utf-8') as f:
        return f.readlines()


def append_to_vault(vault_path: str, lines: List[str]) -> None:
    if not lines:
        return
    with open(vault_path, 'a', encoding='utf-8') as f:
        for line in lines:
            f.write(line if line.endswith('\n') else line + '\n')


def embed_texts(texts: List[str]) -> torch.Tensor:
    if not texts:
        return torch.empty((0,))
    vectors: List[List[float]] = []
    for content in texts:
        resp = ollama.embeddings(model='nomic-embed-text', prompt=content)
        vectors.append(resp["embedding"])
    return torch.tensor(vectors)


def get_relevant_context(rewritten_input: str, vault_embeddings: torch.Tensor, vault_content: List[str], top_k: int = 3) -> List[str]:
    import time
    
    if vault_embeddings is None or vault_embeddings.nelement() == 0:
        print("vault_embeddings ä¸ºç©ºï¼Œè·³è¿‡æ£€ç´¢")
        return []
    
    print(f"å¼€å§‹æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œvault_content é•¿åº¦: {len(vault_content)}")
    
    # æµ‹é‡å‘é‡åµŒå…¥ç”Ÿæˆæ—¶é—´
    embedding_start = time.time()
    
    # æ£€æŸ¥ç¼“å­˜
    if rewritten_input in app_state.query_embedding_cache:
        input_embedding = app_state.query_embedding_cache[rewritten_input]
        print(f"ğŸ“Š ä½¿ç”¨ç¼“å­˜çš„å‘é‡åµŒå…¥")
    else:
        input_embedding = ollama.embeddings(model='nomic-embed-text', prompt=rewritten_input)["embedding"]
        app_state.query_embedding_cache[rewritten_input] = input_embedding
        print(f"ğŸ“Š ç”Ÿæˆæ–°çš„å‘é‡åµŒå…¥å¹¶ç¼“å­˜")
    
    embedding_time = time.time() - embedding_start
    print(f"ğŸ“Š å‘é‡åµŒå…¥å¤„ç†è€—æ—¶: {embedding_time:.2f}ç§’")
    
    # æµ‹é‡ç›¸ä¼¼åº¦è®¡ç®—æ—¶é—´
    similarity_start = time.time()
    cos_scores = torch.cosine_similarity(torch.tensor(input_embedding).unsqueeze(0), vault_embeddings)
    top_k = min(top_k, len(cos_scores))
    top_indices = torch.topk(cos_scores, k=top_k)[1].tolist()
    similarity_time = time.time() - similarity_start
    print(f"ğŸ” ç›¸ä¼¼åº¦è®¡ç®—è€—æ—¶: {similarity_time:.2f}ç§’")
    
    result = [vault_content[idx].strip() for idx in top_indices]
    print(f"æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(result)} ä¸ªç›¸å…³ç‰‡æ®µ")
    return result


def rewrite_query(user_input: str, conversation_history: List[Dict[str, str]], client: OpenAI, model: str) -> str:
    context = "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_history[-2:]])
    prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯å†å²ï¼Œé‡æ–°ç»„ç»‡å¹¶æ”¹å†™ä¸‹é¢çš„ç”¨æˆ·æŸ¥è¯¢ã€‚
        æ”¹å†™åçš„æŸ¥è¯¢åº”è¯¥ï¼š

        - ä¿ç•™åŸæŸ¥è¯¢çš„æ ¸å¿ƒæ„å›¾å’Œå«ä¹‰
        - è®©æŸ¥è¯¢æ›´æ¸…æ™°ã€æ›´å…·ä½“ï¼Œä¾¿äºæ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯
        - ä¸è¦å¼•å…¥ä¸åŸæŸ¥è¯¢æ— å…³çš„æ–°è¯é¢˜
        - è¯·åªä¸“æ³¨äºé‡æ–°ç»„ç»‡è¯­è¨€ï¼Œä¸è¦å°è¯•å›ç­”åŸæŸ¥è¯¢çš„é—®é¢˜

        è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚ï¼Œä»…è¿”å›æ”¹å†™åçš„æŸ¥è¯¢æ–‡æœ¬ï¼Œä¸è¦ä»»ä½•é¢å¤–è§£é‡Šæˆ–æ ¼å¼ã€‚

        å¯¹è¯å†å²ï¼š
        {context}

        åŸå§‹æŸ¥è¯¢ï¼š[{user_input}]

        æ”¹å†™åçš„æŸ¥è¯¢ï¼š
        """
    print(f"å¼€å§‹è°ƒç”¨ rewrite_queryï¼Œå†å²é•¿åº¦: {len(conversation_history)}")
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "system", "content": prompt}],
        max_tokens=2000,
    )
    result = response.choices[0].message.content.strip()
    print(f"rewrite_query å®Œæˆï¼Œç»“æœ: {result}")
    return result


def rewrite_query_stream(user_input: str, conversation_history: List[Dict[str, str]], client: OpenAI, model: str) -> Iterator[str]:
    """æµå¼ç‰ˆæœ¬çš„æŸ¥è¯¢é‡å†™å‡½æ•°"""
    context = "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_history[-2:]])
    prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯å†å²ï¼Œé‡æ–°ç»„ç»‡å¹¶æ”¹å†™ä¸‹é¢çš„ç”¨æˆ·æŸ¥è¯¢ã€‚
        æ”¹å†™åçš„æŸ¥è¯¢åº”è¯¥ï¼š

        - ä¿ç•™åŸæŸ¥è¯¢çš„æ ¸å¿ƒæ„å›¾å’Œå«ä¹‰
        - è®©æŸ¥è¯¢æ›´æ¸…æ™°ã€æ›´å…·ä½“ï¼Œä¾¿äºæ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯
        - ä¸è¦å¼•å…¥ä¸åŸæŸ¥è¯¢æ— å…³çš„æ–°è¯é¢˜
        - è¯·åªä¸“æ³¨äºé‡æ–°ç»„ç»‡è¯­è¨€ï¼Œä¸è¦å°è¯•å›ç­”åŸæŸ¥è¯¢çš„é—®é¢˜

        è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚ï¼Œä»…è¿”å›æ”¹å†™åçš„æŸ¥è¯¢æ–‡æœ¬ï¼Œä¸è¦ä»»ä½•é¢å¤–è§£é‡Šæˆ–æ ¼å¼ã€‚

        å¯¹è¯å†å²ï¼š
        {context}

        åŸå§‹æŸ¥è¯¢ï¼š[{user_input}]

        æ”¹å†™åçš„æŸ¥è¯¢ï¼š
        """
    print(f"å¼€å§‹è°ƒç”¨ rewrite_query_streamï¼Œå†å²é•¿åº¦: {len(conversation_history)}")
    
    stream = client.chat.completions.create(
        model=model,
        messages=[{"role": "system", "content": prompt}],
        max_tokens=2000,
        stream=True,
    )
    
    collected = []
    for event in stream:
        delta = getattr(event.choices[0].delta, 'content', None)
        if delta:
            collected.append(delta)
            yield delta
    
    result = "".join(collected).strip()
    print(f"rewrite_query_stream å®Œæˆï¼Œç»“æœ: {result}")


#
# def rag_chat(user_input: str, system_message: str, conversation_history: List[Dict[str, str]],
#              vault_embeddings: torch.Tensor, vault_content: List[str], client: OpenAI, model: str) -> str:
#     conversation_history.append({"role": "user", "content": user_input})
#
#     rewritten_query = user_input if len(conversation_history) <= 1 else rewrite_query(user_input, conversation_history, client, model)
#
#     relevant_context = get_relevant_context(rewritten_query, vault_embeddings, vault_content)
#     context_str = "\n".join(relevant_context) if relevant_context else ""
#
#     user_input_with_context = user_input if not context_str else user_input + "\n\nRelevant Context:\n" + context_str
#     conversation_history[-1]["content"] = user_input_with_context
#
#     messages = [
#         {"role": "system", "content": system_message},
#         *conversation_history
#     ]
#
#     response = client.chat.completions.create(
#         model=model,
#         messages=messages,
#         max_tokens=2000,
#     )
#     answer = response.choices[0].message.content
#     conversation_history.append({"role": "assistant", "content": answer})
#     return answer
#

def rag_chat_stream(user_input: str, system_message: str, conversation_history: List[Dict[str, str]],
                    vault_embeddings: torch.Tensor, vault_content: List[str], client: OpenAI, model: str) -> Iterator[str]:
    """Yield assistant content chunks as they stream in, and update history when done."""
    import time
    start_time = time.time()
    
    conversation_history.append({"role": "user", "content": user_input})

    # å¦‚æœæ˜¯å¤šè½®å¯¹è¯ï¼Œå…ˆæµå¼é‡å†™æŸ¥è¯¢
    # if len(conversation_history) > 1:
    #     print("å¤šè½®å¯¹è¯ï¼Œå¼€å§‹æµå¼é‡å†™æŸ¥è¯¢...")
    #     yield "<think>æ­£åœ¨æ ¹æ®å¯¹è¯å†å²é‡å†™æŸ¥è¯¢ä»¥æ›´å¥½åœ°æ£€ç´¢ç›¸å…³ä¿¡æ¯...</think>"
    #
    #     """æµå¼ç‰ˆæœ¬çš„æŸ¥è¯¢é‡å†™å‡½æ•°"""
    #     context = "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_history[-2:]])
    #     prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯å†å²ï¼Œé‡æ–°ç»„ç»‡å¹¶æ”¹å†™ä¸‹é¢çš„ç”¨æˆ·æŸ¥è¯¢ã€‚
    #             æ”¹å†™åçš„æŸ¥è¯¢åº”è¯¥ï¼š
    #
    #             - ä¿ç•™åŸæŸ¥è¯¢çš„æ ¸å¿ƒæ„å›¾å’Œå«ä¹‰
    #             - è®©æŸ¥è¯¢æ›´æ¸…æ™°ã€æ›´å…·ä½“ï¼Œä¾¿äºæ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯
    #             - ä¸è¦å¼•å…¥ä¸åŸæŸ¥è¯¢æ— å…³çš„æ–°è¯é¢˜
    #             - è¯·åªä¸“æ³¨äºé‡æ–°ç»„ç»‡è¯­è¨€ï¼Œä¸è¦å°è¯•å›ç­”åŸæŸ¥è¯¢çš„é—®é¢˜
    #
    #             è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚ï¼Œä»…è¿”å›æ”¹å†™åçš„æŸ¥è¯¢æ–‡æœ¬ï¼Œä¸è¦ä»»ä½•é¢å¤–è§£é‡Šæˆ–æ ¼å¼ã€‚
    #
    #             å¯¹è¯å†å²ï¼š
    #             {context}
    #
    #             åŸå§‹æŸ¥è¯¢ï¼š[{user_input}]
    #
    #             æ”¹å†™åçš„æŸ¥è¯¢ï¼š
    #             """
    #     print(f"å¼€å§‹è°ƒç”¨ rewrite_query_streamï¼Œå†å²é•¿åº¦: {len(conversation_history)}")
    #
    #     if app_state.model_loaded:
    #         print(f"ğŸ”„ æŸ¥è¯¢é‡å†™ä½¿ç”¨é¢„åŠ è½½æ¨¡å‹")
    #     else:
    #         print(f"ğŸ”„ æŸ¥è¯¢é‡å†™é¦–æ¬¡åŠ è½½æ¨¡å‹")
    #
    #     stream = client.chat.completions.create(
    #         model=model,
    #         messages=[{"role": "system", "content": prompt}],
    #         max_tokens=2000,
    #         stream=True,
    #     )
    #
    #     collected = []
    #     for event in stream:
    #         delta = getattr(event.choices[0].delta, 'content', None)
    #         if delta:
    #             collected.append(delta)
    #             yield delta
    #
    #     rewritten_query = "".join(collected).strip()
    #     print(f"æŸ¥è¯¢é‡å†™å®Œæˆ: {rewritten_query}")
    # else:
    #     rewritten_query = user_input
    rewritten_query = user_input
    # æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
    yield "<think>æ­£åœ¨æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯...</think>"
    retrieval_start = time.time()
    relevant_context = get_relevant_context(rewritten_query, vault_embeddings, vault_content)
    retrieval_time = time.time() - retrieval_start
    print(f"ğŸ” å‘é‡æ£€ç´¢è€—æ—¶: {retrieval_time:.2f}ç§’")
    # åˆå¹¶ä¸Šä¸‹æ–‡å¹¶åšç¡¬é˜ˆå€¼è£å‰ªï¼Œé¿å…ä¸åŒæŸ¥è¯¢å› ä¸ºä¸Šä¸‹æ–‡é•¿åº¦å¤§å¹…åº¦æ”¾å¤§æ¨ç†æ—¶å»¶
    context_str = "\n".join(relevant_context) if relevant_context else ""
    MAX_CONTEXT_CHARS = int(os.environ.get("MAX_CONTEXT_CHARS", "1200"))
    if len(context_str) > MAX_CONTEXT_CHARS:
        context_str = context_str[:MAX_CONTEXT_CHARS]
    
    if context_str:
        yield f"<think>æ‰¾åˆ° {len(relevant_context)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ</think>"
        user_input_with_context = user_input + "\n\nRelevant Context:\n" + context_str
    else:
        yield "<think>æœªæ‰¾åˆ°ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå°†ç›´æ¥å›ç­”</think>"
        user_input_with_context = user_input
    conversation_history[-1]["content"] = user_input_with_context

    messages = [
        {"role": "system", "content": system_message},
        *conversation_history
    ]

    # å¼€å§‹ç”Ÿæˆå›ç­”
    yield "<think>æ­£åœ¨ç”ŸæˆAIåˆ†æå›ç­”...</think>"
    
    generation_start = time.time()
    if app_state.model_loaded:
        print(f"ğŸš€ å¼€å§‹è°ƒç”¨æ¨¡å‹ç”Ÿæˆ... (æ¨¡å‹å·²é¢„åŠ è½½)")
    else:
        print(f"ğŸš€ å¼€å§‹è°ƒç”¨æ¨¡å‹ç”Ÿæˆ... (é¦–æ¬¡åŠ è½½)")
    # é€šè¿‡ extra_body ä¼ é€’ç»™ Ollamaï¼Œä¿æŒæ¨¡å‹å¸¸é©»å¹¶é™åˆ¶ä¸Šä¸‹æ–‡
    stream = client.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=int(os.environ.get("MAX_GENERATE_TOKENS", "800")),
        stream=True,
        extra_body={
            "keep_alive": "30m",
            # æ ¹æ®æœºå™¨è°ƒæ•´ï¼Œè¾ƒå°çš„ä¸Šä¸‹æ–‡æ›´å¿«ï¼›å¦‚éœ€æ›´å¤§å¯æé«˜
            "options": {
                "num_ctx": int(os.environ.get("NUM_CTX", "2048")),
                # æ§åˆ¶ç”Ÿæˆé•¿åº¦ï¼Œé¿å…é•¿å›ç­”å¯¼è‡´è€—æ—¶ä¸Šå‡
                "num_predict": int(os.environ.get("NUM_PREDICT", "800")),
                # åˆç†åˆ©ç”¨ CPU çº¿ç¨‹
                "num_threads": max(1, __import__('os').cpu_count() or 1)
            }
        },
    )

    # æ›´ç²¾ç¡®çš„é¦– token å»¶è¿Ÿä¸æ€»ä½“è€—æ—¶
    ttft = None
    collected = []
    for event in stream:
        delta = getattr(event.choices[0].delta, 'content', None)
        if delta:
            if ttft is None:
                ttft = time.time() - generation_start
                print(f"âš¡ é¦–tokenå»¶è¿Ÿ(TTFT): {ttft:.2f}ç§’")
            collected.append(delta)
            yield delta
    final_answer = "".join(collected)
    conversation_history.append({"role": "assistant", "content": final_answer})
    total_time = time.time() - start_time
    gen_time = time.time() - generation_start
    print(f"ğŸ¤– æ¨¡å‹ç”Ÿæˆè€—æ—¶: {gen_time:.2f}ç§’")
    print(f"ğŸ¯ æ€»è€—æ—¶: {total_time:.2f}ç§’")


class AppState:
    def __init__(self) -> None:
        self.vault_content: List[str] = []
        self.vault_embeddings: torch.Tensor = torch.empty((0,))
        self.histories: Dict[str, List[Dict[str, str]]] = {}
        self.query_embedding_cache: Dict[str, List[float]] = {}  # ç¼“å­˜æŸ¥è¯¢å‘é‡åµŒå…¥
        self.model_loaded: bool = False  # æ ‡è®°æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        self.system_message: str = (
            "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œæ“…é•¿ä»ç»™å®šæ–‡æœ¬ä¸­æå–æœ€æœ‰ç”¨çš„ä¿¡æ¯ï¼Œå¹¶ç»“åˆä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\n"
            "è¯·å§‹ç»ˆä½¿ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè¯­è¨€è¦æ¸…æ™°ã€ç®€æ´ã€ä¸“ä¸šã€‚\n"
            "å¦‚æœç”¨æˆ·çš„é—®é¢˜æ˜¯ä¸­æ–‡ï¼Œä½ çš„å›ç­”ä¹Ÿå¿…é¡»æ˜¯ä¸­æ–‡ã€‚\n"
            "å¦‚æœç”¨æˆ·çš„é—®é¢˜ä¸­åŒ…å«ä¸­è‹±æ–‡æ··åˆï¼Œä½ ä»ç„¶ä¼˜å…ˆç”¨ä¸­æ–‡å›ç­”ã€‚"
        )
        self.client: OpenAI = OpenAI(base_url='http://localhost:11434/v1', api_key='llama3')


app_state = AppState()


def initialize_state_on_startup() -> None:
    print("=" * 50)
    print("ğŸš€ æ­£åœ¨å¯åŠ¨ RAG æœåŠ¡...")
    print("=" * 50)
    
    # é¢„åŠ è½½æ¨¡å‹
    print("\nğŸ”¥ é¢„åŠ è½½æ¨¡å‹...")
    try:
        import time
        warmup_start = time.time()
        
        # å‘é€ä¸€ä¸ªç®€å•çš„è¯·æ±‚æ¥é¢„åŠ è½½æ¨¡å‹
        test_response = app_state.client.chat.completions.create(
            model=DEFAULT_MODEL,
            messages=[{"role": "user", "content": "ä½ å¥½"}],
            max_tokens=10,
        )
        
        warmup_time = time.time() - warmup_start
        print(f"âœ… æ¨¡å‹é¢„åŠ è½½å®Œæˆï¼Œè€—æ—¶: {warmup_time:.2f}ç§’")
        print(f"   æ¨¡å‹: {DEFAULT_MODEL}")
        print(f"   æ¨¡å‹å“åº”æµ‹è¯•: {test_response.choices[0].message.content}")
        app_state.model_loaded = True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹é¢„åŠ è½½å¤±è´¥: {str(e)}")
        print("   è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ")
        raise
    
    # æ£€æŸ¥embeddingæ¨¡å‹
    print("\nğŸ” æ£€æŸ¥å‘é‡åµŒå…¥æ¨¡å‹...")
    try:
        test_embedding = ollama.embeddings(model='nomic-embed-text', prompt="test")
        embedding_dim = len(test_embedding["embedding"])
        print(f"âœ… å‘é‡åµŒå…¥æ¨¡å‹è¿æ¥æˆåŠŸ - æ¨¡å‹: nomic-embed-text")
        print(f"   å‘é‡ç»´åº¦: {embedding_dim}")
    except Exception as e:
        print(f"âŒ å‘é‡åµŒå…¥æ¨¡å‹è¿æ¥å¤±è´¥: {str(e)}")
        print("   è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œä¸”åŒ…å« nomic-embed-text æ¨¡å‹")
        raise
    
    # åŠ è½½å‘é‡æ•°æ®åº“
    print("\nğŸ“š åŠ è½½å‘é‡æ•°æ®åº“...")
    contents = read_vault_lines(VAULT_PATH)
    print(f"   è¯»å–æ–‡ä»¶: {VAULT_PATH}")
    print(f"   æ–‡æ¡£æ•°é‡: {len(contents)}")
    
    if len(contents) == 0:
        print("âš ï¸  è­¦å‘Š: å‘é‡æ•°æ®åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£")
        app_state.vault_content = []
        app_state.vault_embeddings = torch.empty((0,))
    else:
        print(f"   æ–‡æ¡£æ€»é•¿åº¦: {sum(len(content) for content in contents)} å­—ç¬¦")
        
        # ç”Ÿæˆå‘é‡åµŒå…¥
        print("\nğŸ”„ ç”Ÿæˆå‘é‡åµŒå…¥...")
        try:
            app_state.vault_content = contents
            app_state.vault_embeddings = embed_texts(app_state.vault_content)
            print(f"âœ… å‘é‡åµŒå…¥ç”ŸæˆæˆåŠŸ")
            print(f"   å‘é‡çŸ©é˜µå½¢çŠ¶: {app_state.vault_embeddings.shape}")
            print(f"   å‘é‡æ•°é‡: {app_state.vault_embeddings.shape[0]}")
            print(f"   å‘é‡ç»´åº¦: {app_state.vault_embeddings.shape[1]}")
        except Exception as e:
            print(f"âŒ å‘é‡åµŒå…¥ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise
    
    print("\n" + "=" * 50)
    print("ğŸ‰ RAG æœåŠ¡å¯åŠ¨æˆåŠŸï¼")
    print("=" * 50)


