import logging
import os
import io
from typing import List, Dict, Optional, Iterator

import torch
import ollama
from PyPDF2 import PdfReader
from openai import OpenAI


VAULT_PATH = os.environ.get("VAULT_PATH", "vault.txt")
# DEFAULT_MODEL = os.environ.get("OLLAMA_MODEL", "deepseek-r1:7b")
DEFAULT_MODEL = os.environ.get("OLLAMA_MODEL", "llama3:latest")


def read_vault_lines(vault_path: str) -> List[str]:
    if not os.path.exists(vault_path):
        return []
    with open(vault_path, 'r', encoding='utf-8') as f:
        return f.readlines()


def append_to_vault(vault_path: str, lines: List[str]) -> None:
    if not lines:
        return
    with open(vault_path, 'a', encoding='utf-8') as f:
        for line in lines:
            f.write(line if line.endswith('\n') else line + '\n')


def embed_texts(texts: List[str]) -> torch.Tensor:
    if not texts:
        return torch.empty((0,))
    vectors: List[List[float]] = []
    for content in texts:
        resp = ollama.embeddings(model='nomic-embed-text', prompt=content)
        vectors.append(resp["embedding"])
    return torch.tensor(vectors)


def get_relevant_context(rewritten_input: str, vault_embeddings: torch.Tensor, vault_content: List[str], top_k: int = 3) -> List[str]:
    if vault_embeddings is None or vault_embeddings.nelement() == 0:
        print("vault_embeddings ä¸ºç©ºï¼Œè·³è¿‡æ£€ç´¢")
        return []
    print(f"å¼€å§‹æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œvault_content é•¿åº¦: {len(vault_content)}")
    input_embedding = ollama.embeddings(model='nomic-embed-text', prompt=rewritten_input)["embedding"]
    cos_scores = torch.cosine_similarity(torch.tensor(input_embedding).unsqueeze(0), vault_embeddings)
    top_k = min(top_k, len(cos_scores))
    top_indices = torch.topk(cos_scores, k=top_k)[1].tolist()
    result = [vault_content[idx].strip() for idx in top_indices]
    print(f"æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(result)} ä¸ªç›¸å…³ç‰‡æ®µ")
    return result


def rewrite_query(user_input: str, conversation_history: List[Dict[str, str]], client: OpenAI, model: str) -> str:
    context = "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_history[-2:]])
    prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯å†å²ï¼Œé‡æ–°ç»„ç»‡å¹¶æ”¹å†™ä¸‹é¢çš„ç”¨æˆ·æŸ¥è¯¢ã€‚
        æ”¹å†™åçš„æŸ¥è¯¢åº”è¯¥ï¼š

        - ä¿ç•™åŸæŸ¥è¯¢çš„æ ¸å¿ƒæ„å›¾å’Œå«ä¹‰
        - è®©æŸ¥è¯¢æ›´æ¸…æ™°ã€æ›´å…·ä½“ï¼Œä¾¿äºæ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯
        - ä¸è¦å¼•å…¥ä¸åŸæŸ¥è¯¢æ— å…³çš„æ–°è¯é¢˜
        - è¯·åªä¸“æ³¨äºé‡æ–°ç»„ç»‡è¯­è¨€ï¼Œä¸è¦å°è¯•å›ç­”åŸæŸ¥è¯¢çš„é—®é¢˜

        è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚ï¼Œä»…è¿”å›æ”¹å†™åçš„æŸ¥è¯¢æ–‡æœ¬ï¼Œä¸è¦ä»»ä½•é¢å¤–è§£é‡Šæˆ–æ ¼å¼ã€‚

        å¯¹è¯å†å²ï¼š
        {context}

        åŸå§‹æŸ¥è¯¢ï¼š[{user_input}]

        æ”¹å†™åçš„æŸ¥è¯¢ï¼š
        """
    print(f"å¼€å§‹è°ƒç”¨ rewrite_queryï¼Œå†å²é•¿åº¦: {len(conversation_history)}")
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "system", "content": prompt}],
        max_tokens=2000,
    )
    result = response.choices[0].message.content.strip()
    print(f"rewrite_query å®Œæˆï¼Œç»“æœ: {result}")
    return result


def rewrite_query_stream(user_input: str, conversation_history: List[Dict[str, str]], client: OpenAI, model: str) -> Iterator[str]:
    """æµå¼ç‰ˆæœ¬çš„æŸ¥è¯¢é‡å†™å‡½æ•°"""
    context = "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_history[-2:]])
    prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯å†å²ï¼Œé‡æ–°ç»„ç»‡å¹¶æ”¹å†™ä¸‹é¢çš„ç”¨æˆ·æŸ¥è¯¢ã€‚
        æ”¹å†™åçš„æŸ¥è¯¢åº”è¯¥ï¼š

        - ä¿ç•™åŸæŸ¥è¯¢çš„æ ¸å¿ƒæ„å›¾å’Œå«ä¹‰
        - è®©æŸ¥è¯¢æ›´æ¸…æ™°ã€æ›´å…·ä½“ï¼Œä¾¿äºæ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯
        - ä¸è¦å¼•å…¥ä¸åŸæŸ¥è¯¢æ— å…³çš„æ–°è¯é¢˜
        - è¯·åªä¸“æ³¨äºé‡æ–°ç»„ç»‡è¯­è¨€ï¼Œä¸è¦å°è¯•å›ç­”åŸæŸ¥è¯¢çš„é—®é¢˜

        è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚ï¼Œä»…è¿”å›æ”¹å†™åçš„æŸ¥è¯¢æ–‡æœ¬ï¼Œä¸è¦ä»»ä½•é¢å¤–è§£é‡Šæˆ–æ ¼å¼ã€‚

        å¯¹è¯å†å²ï¼š
        {context}

        åŸå§‹æŸ¥è¯¢ï¼š[{user_input}]

        æ”¹å†™åçš„æŸ¥è¯¢ï¼š
        """
    print(f"å¼€å§‹è°ƒç”¨ rewrite_query_streamï¼Œå†å²é•¿åº¦: {len(conversation_history)}")
    
    stream = client.chat.completions.create(
        model=model,
        messages=[{"role": "system", "content": prompt}],
        max_tokens=2000,
        stream=True,
    )
    
    collected = []
    for event in stream:
        delta = getattr(event.choices[0].delta, 'content', None)
        if delta:
            collected.append(delta)
            yield delta
    
    result = "".join(collected).strip()
    print(f"rewrite_query_stream å®Œæˆï¼Œç»“æœ: {result}")


#
# def rag_chat(user_input: str, system_message: str, conversation_history: List[Dict[str, str]],
#              vault_embeddings: torch.Tensor, vault_content: List[str], client: OpenAI, model: str) -> str:
#     conversation_history.append({"role": "user", "content": user_input})
#
#     rewritten_query = user_input if len(conversation_history) <= 1 else rewrite_query(user_input, conversation_history, client, model)
#
#     relevant_context = get_relevant_context(rewritten_query, vault_embeddings, vault_content)
#     context_str = "\n".join(relevant_context) if relevant_context else ""
#
#     user_input_with_context = user_input if not context_str else user_input + "\n\nRelevant Context:\n" + context_str
#     conversation_history[-1]["content"] = user_input_with_context
#
#     messages = [
#         {"role": "system", "content": system_message},
#         *conversation_history
#     ]
#
#     response = client.chat.completions.create(
#         model=model,
#         messages=messages,
#         max_tokens=2000,
#     )
#     answer = response.choices[0].message.content
#     conversation_history.append({"role": "assistant", "content": answer})
#     return answer
#

def rag_chat_stream(user_input: str, system_message: str, conversation_history: List[Dict[str, str]],
                    vault_embeddings: torch.Tensor, vault_content: List[str], client: OpenAI, model: str) -> Iterator[str]:
    """Yield assistant content chunks as they stream in, and update history when done."""
    conversation_history.append({"role": "user", "content": user_input})

    # å¦‚æœæ˜¯å¤šè½®å¯¹è¯ï¼Œå…ˆæµå¼é‡å†™æŸ¥è¯¢
    if len(conversation_history) > 1:
        print("å¤šè½®å¯¹è¯ï¼Œå¼€å§‹æµå¼é‡å†™æŸ¥è¯¢...")
        yield "<think>æ­£åœ¨æ ¹æ®å¯¹è¯å†å²é‡å†™æŸ¥è¯¢ä»¥æ›´å¥½åœ°æ£€ç´¢ç›¸å…³ä¿¡æ¯...</think>"

        """æµå¼ç‰ˆæœ¬çš„æŸ¥è¯¢é‡å†™å‡½æ•°"""
        context = "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_history[-2:]])
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯å†å²ï¼Œé‡æ–°ç»„ç»‡å¹¶æ”¹å†™ä¸‹é¢çš„ç”¨æˆ·æŸ¥è¯¢ã€‚
                æ”¹å†™åçš„æŸ¥è¯¢åº”è¯¥ï¼š

                - ä¿ç•™åŸæŸ¥è¯¢çš„æ ¸å¿ƒæ„å›¾å’Œå«ä¹‰
                - è®©æŸ¥è¯¢æ›´æ¸…æ™°ã€æ›´å…·ä½“ï¼Œä¾¿äºæ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯
                - ä¸è¦å¼•å…¥ä¸åŸæŸ¥è¯¢æ— å…³çš„æ–°è¯é¢˜
                - è¯·åªä¸“æ³¨äºé‡æ–°ç»„ç»‡è¯­è¨€ï¼Œä¸è¦å°è¯•å›ç­”åŸæŸ¥è¯¢çš„é—®é¢˜

                è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚ï¼Œä»…è¿”å›æ”¹å†™åçš„æŸ¥è¯¢æ–‡æœ¬ï¼Œä¸è¦ä»»ä½•é¢å¤–è§£é‡Šæˆ–æ ¼å¼ã€‚

                å¯¹è¯å†å²ï¼š
                {context}

                åŸå§‹æŸ¥è¯¢ï¼š[{user_input}]

                æ”¹å†™åçš„æŸ¥è¯¢ï¼š
                """
        print(f"å¼€å§‹è°ƒç”¨ rewrite_query_streamï¼Œå†å²é•¿åº¦: {len(conversation_history)}")

        stream = client.chat.completions.create(
            model=model,
            messages=[{"role": "system", "content": prompt}],
            max_tokens=2000,
            stream=True,
        )

        collected = []
        for event in stream:
            delta = getattr(event.choices[0].delta, 'content', None)
            if delta:
                collected.append(delta)
                yield delta

        rewritten_query = "".join(collected).strip()
        print(f"æŸ¥è¯¢é‡å†™å®Œæˆ: {rewritten_query}")
    else:
        rewritten_query = user_input

    # æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
    yield "<think>æ­£åœ¨æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯...</think>"
    relevant_context = get_relevant_context(rewritten_query, vault_embeddings, vault_content)
    context_str = "\n".join(relevant_context) if relevant_context else ""
    
    if context_str:
        yield f"<think>æ‰¾åˆ° {len(relevant_context)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ</think>"
    else:
        yield "<think>æœªæ‰¾åˆ°ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯</think>"

    user_input_with_context = user_input if not context_str else user_input + "\n\nRelevant Context:\n" + context_str
    conversation_history[-1]["content"] = user_input_with_context

    messages = [
        {"role": "system", "content": system_message},
        *conversation_history
    ]

    # å¼€å§‹ç”Ÿæˆå›ç­”
    yield "<think>æ­£åœ¨ç”Ÿæˆç»„ç»‡ä¹‹åçš„å›ç­”...</think>"
    
    stream = client.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=2000,
        stream=True,
    )

    collected = []
    for event in stream:
        delta = getattr(event.choices[0].delta, 'content', None)
        if delta:
            collected.append(delta)
            yield delta
    final_answer = "".join(collected)
    conversation_history.append({"role": "assistant", "content": final_answer})


class AppState:
    def __init__(self) -> None:
        self.vault_content: List[str] = []
        self.vault_embeddings: torch.Tensor = torch.empty((0,))
        self.histories: Dict[str, List[Dict[str, str]]] = {}
        self.system_message: str = (
            "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œæ“…é•¿ä»ç»™å®šæ–‡æœ¬ä¸­æå–æœ€æœ‰ç”¨çš„ä¿¡æ¯ï¼Œå¹¶ç»“åˆä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\n"
            "è¯·å§‹ç»ˆä½¿ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè¯­è¨€è¦æ¸…æ™°ã€ç®€æ´ã€ä¸“ä¸šã€‚\n"
            "å¦‚æœç”¨æˆ·çš„é—®é¢˜æ˜¯ä¸­æ–‡ï¼Œä½ çš„å›ç­”ä¹Ÿå¿…é¡»æ˜¯ä¸­æ–‡ã€‚\n"
            "å¦‚æœç”¨æˆ·çš„é—®é¢˜ä¸­åŒ…å«ä¸­è‹±æ–‡æ··åˆï¼Œä½ ä»ç„¶ä¼˜å…ˆç”¨ä¸­æ–‡å›ç­”ã€‚"
        )
        self.client: OpenAI = OpenAI(base_url='http://localhost:11434/v1', api_key='deepseek-r1:7b')


app_state = AppState()


def initialize_state_on_startup() -> None:
    print("=" * 50)
    print("ğŸš€ æ­£åœ¨å¯åŠ¨ RAG æœåŠ¡...")
    print("=" * 50)
    
    # æ£€æŸ¥æ¨¡å‹æœåŠ¡
    print("\nğŸ“‹ æ£€æŸ¥æ¨¡å‹æœåŠ¡çŠ¶æ€...")
    try:
        # æµ‹è¯•æ¨¡å‹è¿æ¥
        test_response = app_state.client.chat.completions.create(
            model=DEFAULT_MODEL,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=10,
        )
        print(f"âœ… æ¨¡å‹æœåŠ¡è¿æ¥æˆåŠŸ - æ¨¡å‹: {DEFAULT_MODEL}")
        print(f"   æ¨¡å‹å“åº”æµ‹è¯•: {test_response.choices[0].message.content}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹æœåŠ¡è¿æ¥å¤±è´¥: {str(e)}")
        print("   è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ")
        raise
    
    # æ£€æŸ¥embeddingæ¨¡å‹
    print("\nğŸ” æ£€æŸ¥å‘é‡åµŒå…¥æ¨¡å‹...")
    try:
        test_embedding = ollama.embeddings(model='nomic-embed-text', prompt="test")
        embedding_dim = len(test_embedding["embedding"])
        print(f"âœ… å‘é‡åµŒå…¥æ¨¡å‹è¿æ¥æˆåŠŸ - æ¨¡å‹: nomic-embed-text")
        print(f"   å‘é‡ç»´åº¦: {embedding_dim}")
    except Exception as e:
        print(f"âŒ å‘é‡åµŒå…¥æ¨¡å‹è¿æ¥å¤±è´¥: {str(e)}")
        print("   è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œä¸”åŒ…å« nomic-embed-text æ¨¡å‹")
        raise
    
    # åŠ è½½å‘é‡æ•°æ®åº“
    print("\nğŸ“š åŠ è½½å‘é‡æ•°æ®åº“...")
    contents = read_vault_lines(VAULT_PATH)
    print(f"   è¯»å–æ–‡ä»¶: {VAULT_PATH}")
    print(f"   æ–‡æ¡£æ•°é‡: {len(contents)}")
    
    if len(contents) == 0:
        print("âš ï¸  è­¦å‘Š: å‘é‡æ•°æ®åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£")
        app_state.vault_content = []
        app_state.vault_embeddings = torch.empty((0,))
    else:
        print(f"   æ–‡æ¡£æ€»é•¿åº¦: {sum(len(content) for content in contents)} å­—ç¬¦")
        
        # ç”Ÿæˆå‘é‡åµŒå…¥
        print("\nğŸ”„ ç”Ÿæˆå‘é‡åµŒå…¥...")
        try:
            app_state.vault_content = contents
            app_state.vault_embeddings = embed_texts(app_state.vault_content)
            print(f"âœ… å‘é‡åµŒå…¥ç”ŸæˆæˆåŠŸ")
            print(f"   å‘é‡çŸ©é˜µå½¢çŠ¶: {app_state.vault_embeddings.shape}")
            print(f"   å‘é‡æ•°é‡: {app_state.vault_embeddings.shape[0]}")
            print(f"   å‘é‡ç»´åº¦: {app_state.vault_embeddings.shape[1]}")
        except Exception as e:
            print(f"âŒ å‘é‡åµŒå…¥ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise
    
    print("\n" + "=" * 50)
    print("ğŸ‰ RAG æœåŠ¡å¯åŠ¨æˆåŠŸï¼")
    print("=" * 50)


