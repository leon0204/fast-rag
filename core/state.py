import logging
import os
import io
from typing import List, Dict, Optional, Iterator

import torch
import ollama
from PyPDF2 import PdfReader
from openai import OpenAI

from core.vector_store import vector_store
from config.database import init_database, get_chunk_count


# DEFAULT_MODEL = os.environ.get("OLLAMA_MODEL", "deepseek-r1:7b")
DEFAULT_MODEL = os.environ.get("OLLAMA_MODEL", "llama3:latest")  # å¯ä½¿ç”¨æ›´å°çš„æ¨¡å‹


# vector_store
def get_relevant_context(rewritten_input: str, top_k: int = 3) -> List[str]:
    import time
    
    print(f"å¼€å§‹æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡")
    
    # æµ‹é‡å‘é‡åµŒå…¥ç”Ÿæˆæ—¶é—´
    embedding_start = time.time()
    
    # æ£€æŸ¥ç¼“å­˜
    if rewritten_input in app_state.query_embedding_cache:
        input_embedding = app_state.query_embedding_cache[rewritten_input]
        print(f"ğŸ“Š ä½¿ç”¨ç¼“å­˜çš„å‘é‡åµŒå…¥")
    else:
        resp = ollama.embeddings(model='nomic-embed-text', prompt=rewritten_input)
        # ç¡®ä¿å‘é‡æ˜¯æµ®ç‚¹æ•°åˆ—è¡¨
        input_embedding = [float(x) for x in resp["embedding"]]
        app_state.query_embedding_cache[rewritten_input] = input_embedding
        print(f"ğŸ“Š ç”Ÿæˆæ–°çš„å‘é‡åµŒå…¥å¹¶ç¼“å­˜")
    
    embedding_time = time.time() - embedding_start
    print(f"ğŸ“Š å‘é‡åµŒå…¥å¤„ç†è€—æ—¶: {embedding_time:.2f}ç§’")
    
    # æµ‹é‡å‘é‡æ£€ç´¢æ—¶é—´
    retrieval_start = time.time()
    similar_chunks = vector_store.search_similar(input_embedding, top_k)
    retrieval_time = time.time() - retrieval_start
    print(f"ğŸ” å‘é‡æ£€ç´¢è€—æ—¶: {retrieval_time:.2f}ç§’")
    
    result = [chunk['content'].strip() for chunk in similar_chunks]
    print(f"æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(result)} ä¸ªç›¸å…³ç‰‡æ®µ")
    return result


def rewrite_query(user_input: str, conversation_history: List[Dict[str, str]], client: OpenAI, model: str) -> str:
    context = "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_history[-2:]])
    prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯å†å²ï¼Œé‡æ–°ç»„ç»‡å¹¶æ”¹å†™ä¸‹é¢çš„ç”¨æˆ·æŸ¥è¯¢ã€‚
        æ”¹å†™åçš„æŸ¥è¯¢åº”è¯¥ï¼š

        - ä¿ç•™åŸæŸ¥è¯¢çš„æ ¸å¿ƒæ„å›¾å’Œå«ä¹‰
        - è®©æŸ¥è¯¢æ›´æ¸…æ™°ã€æ›´å…·ä½“ï¼Œä¾¿äºæ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯
        - ä¸è¦å¼•å…¥ä¸åŸæŸ¥è¯¢æ— å…³çš„æ–°è¯é¢˜
        - è¯·åªä¸“æ³¨äºé‡æ–°ç»„ç»‡è¯­è¨€ï¼Œä¸è¦å°è¯•å›ç­”åŸæŸ¥è¯¢çš„é—®é¢˜

        è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚ï¼Œä»…è¿”å›æ”¹å†™åçš„æŸ¥è¯¢æ–‡æœ¬ï¼Œä¸è¦ä»»ä½•é¢å¤–è§£é‡Šæˆ–æ ¼å¼ã€‚

        å¯¹è¯å†å²ï¼š
        {context}

        åŸå§‹æŸ¥è¯¢ï¼š[{user_input}]

        æ”¹å†™åçš„æŸ¥è¯¢ï¼š
        """
    print(f"å¼€å§‹è°ƒç”¨ rewrite_queryï¼Œå†å²é•¿åº¦: {len(conversation_history)}")
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "system", "content": prompt}],
        max_tokens=2000,
    )
    result = response.choices[0].message.content.strip()
    print(f"rewrite_query å®Œæˆï¼Œç»“æœ: {result}")
    return result


def rewrite_query_stream(user_input: str, conversation_history: List[Dict[str, str]], client: OpenAI, model: str) -> Iterator[str]:
    """æµå¼ç‰ˆæœ¬çš„æŸ¥è¯¢é‡å†™å‡½æ•°"""
    context = "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_history[-2:]])
    prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯å†å²ï¼Œé‡æ–°ç»„ç»‡å¹¶æ”¹å†™ä¸‹é¢çš„ç”¨æˆ·æŸ¥è¯¢ã€‚
        æ”¹å†™åçš„æŸ¥è¯¢åº”è¯¥ï¼š

        - ä¿ç•™åŸæŸ¥è¯¢çš„æ ¸å¿ƒæ„å›¾å’Œå«ä¹‰
        - è®©æŸ¥è¯¢æ›´æ¸…æ™°ã€æ›´å…·ä½“ï¼Œä¾¿äºæ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯
        - ä¸è¦å¼•å…¥ä¸åŸæŸ¥è¯¢æ— å…³çš„æ–°è¯é¢˜
        - è¯·åªä¸“æ³¨äºé‡æ–°ç»„ç»‡è¯­è¨€ï¼Œä¸è¦å°è¯•å›ç­”åŸæŸ¥è¯¢çš„é—®é¢˜

        è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚ï¼Œä»…è¿”å›æ”¹å†™åçš„æŸ¥è¯¢æ–‡æœ¬ï¼Œä¸è¦ä»»ä½•é¢å¤–è§£é‡Šæˆ–æ ¼å¼ã€‚

        å¯¹è¯å†å²ï¼š
        {context}

        åŸå§‹æŸ¥è¯¢ï¼š[{user_input}]

        æ”¹å†™åçš„æŸ¥è¯¢ï¼š
        """
    print(f"å¼€å§‹è°ƒç”¨ rewrite_query_streamï¼Œå†å²é•¿åº¦: {len(conversation_history)}")
    
    stream = client.chat.completions.create(
        model=model,
        messages=[{"role": "system", "content": prompt}],
        max_tokens=2000,
        stream=True,
    )
    
    collected = []
    for event in stream:
        delta = getattr(event.choices[0].delta, 'content', None)
        if delta:
            collected.append(delta)
            yield delta
    
    result = "".join(collected).strip()
    print(f"rewrite_query_stream å®Œæˆï¼Œç»“æœ: {result}")


#
# def rag_chat(user_input: str, system_message: str, conversation_history: List[Dict[str, str]],
#              vault_embeddings: torch.Tensor, vault_content: List[str], client: OpenAI, model: str) -> str:
#     conversation_history.append({"role": "user", "content": user_input})
#
#     rewritten_query = user_input if len(conversation_history) <= 1 else rewrite_query(user_input, conversation_history, client, model)
#
#     relevant_context = get_relevant_context(rewritten_query, vault_embeddings, vault_content)
#     context_str = "\n".join(relevant_context) if relevant_context else ""
#
#     user_input_with_context = user_input if not context_str else user_input + "\n\nRelevant Context:\n" + context_str
#     conversation_history[-1]["content"] = user_input_with_context
#
#     messages = [
#         {"role": "system", "content": system_message},
#         *conversation_history
#     ]
#
#     response = client.chat.completions.create(
#         model=model,
#         messages=messages,
#         max_tokens=2000,
#     )
#     answer = response.choices[0].message.content
#     conversation_history.append({"role": "assistant", "content": answer})
#     return answer
#

def rag_chat_stream(user_input: str, system_message: str, conversation_history: List[Dict[str, str]],
                    client: OpenAI, model: str) -> Iterator[str]:
    """Yield assistant content chunks as they stream in, and update history when done."""
    import time
    start_time = time.time()
    
    conversation_history.append({"role": "user", "content": user_input})

    # å¦‚æœæ˜¯å¤šè½®å¯¹è¯ï¼Œå…ˆæµå¼é‡å†™æŸ¥è¯¢
    # if len(conversation_history) > 1:
    #     print("å¤šè½®å¯¹è¯ï¼Œå¼€å§‹æµå¼é‡å†™æŸ¥è¯¢...")
    #     yield "<think>æ­£åœ¨æ ¹æ®å¯¹è¯å†å²é‡å†™æŸ¥è¯¢ä»¥æ›´å¥½åœ°æ£€ç´¢ç›¸å…³ä¿¡æ¯...</think>"
    #
    #     """æµå¼ç‰ˆæœ¬çš„æŸ¥è¯¢é‡å†™å‡½æ•°"""
    #     context = "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_history[-2:]])
    #     prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯å†å²ï¼Œé‡æ–°ç»„ç»‡å¹¶æ”¹å†™ä¸‹é¢çš„ç”¨æˆ·æŸ¥è¯¢ã€‚
    #             æ”¹å†™åçš„æŸ¥è¯¢åº”è¯¥ï¼š
    #
    #             - ä¿ç•™åŸæŸ¥è¯¢çš„æ ¸å¿ƒæ„å›¾å’Œå«ä¹‰
    #             - è®©æŸ¥è¯¢æ›´æ¸…æ™°ã€æ›´å…·ä½“ï¼Œä¾¿äºæ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯
    #             - ä¸è¦å¼•å…¥ä¸åŸæŸ¥è¯¢æ— å…³çš„æ–°è¯é¢˜
    #             - è¯·åªä¸“æ³¨äºé‡æ–°ç»„ç»‡è¯­è¨€ï¼Œä¸è¦å°è¯•å›ç­”åŸæŸ¥è¯¢çš„é—®é¢˜
    #
    #             è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚ï¼Œä»…è¿”å›æ”¹å†™åçš„æŸ¥è¯¢æ–‡æœ¬ï¼Œä¸è¦ä»»ä½•é¢å¤–è§£é‡Šæˆ–æ ¼å¼ã€‚
    #
    #             å¯¹è¯å†å²ï¼š
    #             {context}
    #
    #             åŸå§‹æŸ¥è¯¢ï¼š[{user_input}]
    #
    #             æ”¹å†™åçš„æŸ¥è¯¢ï¼š
    #             """
    #     print(f"å¼€å§‹è°ƒç”¨ rewrite_query_streamï¼Œå†å²é•¿åº¦: {len(conversation_history)}")
    #
    #     if app_state.model_loaded:
    #         print(f"ğŸ”„ æŸ¥è¯¢é‡å†™ä½¿ç”¨é¢„åŠ è½½æ¨¡å‹")
    #     else:
    #         print(f"ğŸ”„ æŸ¥è¯¢é‡å†™é¦–æ¬¡åŠ è½½æ¨¡å‹")
    #
    #     stream = client.chat.completions.create(
    #         model=model,
    #         messages=[{"role": "system", "content": prompt}],
    #         max_tokens=2000,
    #         stream=True,
    #     )
    #
    #     collected = []
    #     for event in stream:
    #         delta = getattr(event.choices[0].delta, 'content', None)
    #         if delta:
    #             collected.append(delta)
    #             yield delta
    #
    #     rewritten_query = "".join(collected).strip()
    #     print(f"æŸ¥è¯¢é‡å†™å®Œæˆ: {rewritten_query}")
    # else:
    #     rewritten_query = user_input
    rewritten_query = user_input
    # æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
    yield "<think>æ­£åœ¨æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯...</think>"
    retrieval_start = time.time()
    relevant_context = get_relevant_context(rewritten_query)
    retrieval_time = time.time() - retrieval_start
    print(f"ğŸ” å‘é‡æ£€ç´¢è€—æ—¶: {retrieval_time:.2f}ç§’")
    # åˆå¹¶ä¸Šä¸‹æ–‡å¹¶åšç¡¬é˜ˆå€¼è£å‰ªï¼Œé¿å…ä¸åŒæŸ¥è¯¢å› ä¸ºä¸Šä¸‹æ–‡é•¿åº¦å¤§å¹…åº¦æ”¾å¤§æ¨ç†æ—¶å»¶
    context_str = "\n".join(relevant_context) if relevant_context else ""
    MAX_CONTEXT_CHARS = int(os.environ.get("MAX_CONTEXT_CHARS", "1200"))
    if len(context_str) > MAX_CONTEXT_CHARS:
        context_str = context_str[:MAX_CONTEXT_CHARS]
    
    if context_str:
        yield f"<think>æ‰¾åˆ° {len(relevant_context)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ</think>"
        user_input_with_context = user_input + "\n\nRelevant Context:\n" + context_str
    else:
        yield "<think>æœªæ‰¾åˆ°ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå°†ç›´æ¥å›ç­”</think>"
        user_input_with_context = user_input
    conversation_history[-1]["content"] = user_input_with_context

    messages = [
        {"role": "system", "content": system_message},
        *conversation_history
    ]

    # å¼€å§‹ç”Ÿæˆå›ç­”
    yield "<think>æ­£åœ¨ç”ŸæˆAIåˆ†æå›ç­”...</think>"
    
    generation_start = time.time()
    if app_state.model_loaded:
        print(f"ğŸš€ å¼€å§‹è°ƒç”¨æ¨¡å‹ç”Ÿæˆ... (æ¨¡å‹å·²é¢„åŠ è½½)")
    else:
        print(f"ğŸš€ å¼€å§‹è°ƒç”¨æ¨¡å‹ç”Ÿæˆ... (é¦–æ¬¡åŠ è½½)")
    # é€šè¿‡ extra_body ä¼ é€’ç»™ Ollamaï¼Œä¿æŒæ¨¡å‹å¸¸é©»å¹¶é™åˆ¶ä¸Šä¸‹æ–‡
    stream = client.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=int(os.environ.get("MAX_GENERATE_TOKENS", "800")),
        stream=True,
        extra_body={
            "keep_alive": "30m",
            # æ ¹æ®æœºå™¨è°ƒæ•´ï¼Œè¾ƒå°çš„ä¸Šä¸‹æ–‡æ›´å¿«ï¼›å¦‚éœ€æ›´å¤§å¯æé«˜
            "options": {
                "num_ctx": int(os.environ.get("NUM_CTX", "2048")),
                # æ§åˆ¶ç”Ÿæˆé•¿åº¦ï¼Œé¿å…é•¿å›ç­”å¯¼è‡´è€—æ—¶ä¸Šå‡
                "num_predict": int(os.environ.get("NUM_PREDICT", "800")),
                # åˆç†åˆ©ç”¨ CPU çº¿ç¨‹
                "num_threads": max(1, __import__('os').cpu_count() or 1)
            }
        },
    )

    # æ›´ç²¾ç¡®çš„é¦– token å»¶è¿Ÿä¸æ€»ä½“è€—æ—¶
    ttft = None
    collected = []
    for event in stream:
        delta = getattr(event.choices[0].delta, 'content', None)
        if delta:
            if ttft is None:
                ttft = time.time() - generation_start
                print(f"âš¡ é¦–tokenå»¶è¿Ÿ(TTFT): {ttft:.2f}ç§’")
            collected.append(delta)
            yield delta
    final_answer = "".join(collected)
    conversation_history.append({"role": "assistant", "content": final_answer})
    total_time = time.time() - start_time
    gen_time = time.time() - generation_start
    print(f"ğŸ¤– æ¨¡å‹ç”Ÿæˆè€—æ—¶: {gen_time:.2f}ç§’")
    print(f"ğŸ¯ æ€»è€—æ—¶: {total_time:.2f}ç§’")


class AppState:
    def __init__(self) -> None:
        self.histories: Dict[str, List[Dict[str, str]]] = {}
        self.query_embedding_cache: Dict[str, List[float]] = {}  # ç¼“å­˜æŸ¥è¯¢å‘é‡åµŒå…¥
        self.model_loaded: bool = False  # æ ‡è®°æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        self.system_message: str = (
            "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œæ“…é•¿ä»ç»™å®šæ–‡æœ¬ä¸­æå–æœ€æœ‰ç”¨çš„ä¿¡æ¯ï¼Œå¹¶ç»“åˆä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\n"
            "è¯·å§‹ç»ˆä½¿ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè¯­è¨€è¦æ¸…æ™°ã€ç®€æ´ã€ä¸“ä¸šã€‚\n"
            "å¦‚æœç”¨æˆ·çš„é—®é¢˜æ˜¯ä¸­æ–‡ï¼Œä½ çš„å›ç­”ä¹Ÿå¿…é¡»æ˜¯ä¸­æ–‡ã€‚\n"
            "å¦‚æœç”¨æˆ·çš„é—®é¢˜ä¸­åŒ…å«ä¸­è‹±æ–‡æ··åˆï¼Œä½ ä»ç„¶ä¼˜å…ˆç”¨ä¸­æ–‡å›ç­”ã€‚"
        )
        self.client: OpenAI = OpenAI(base_url='http://localhost:11434/v1', api_key='llama3')


app_state = AppState()


def initialize_state_on_startup() -> None:
    print("=" * 50)
    print("ğŸš€ æ­£åœ¨å¯åŠ¨ RAG æœåŠ¡...")
    print("=" * 50)
    
    # åˆå§‹åŒ–æ•°æ®åº“
    print("\nğŸ—„ï¸  åˆå§‹åŒ–æ•°æ®åº“...")
    try:
        init_database()
        print("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        print("   è¯·ç¡®ä¿ PostgreSQL æœåŠ¡æ­£åœ¨è¿è¡Œä¸”å·²å®‰è£… pgvector æ‰©å±•")
        raise
    
    # é¢„åŠ è½½æ¨¡å‹
    # print("\nğŸ”¥ é¢„åŠ è½½æ¨¡å‹...")
    # try:
    #     import time
    #     warmup_start = time.time()
        
    #     # å‘é€ä¸€ä¸ªç®€å•çš„è¯·æ±‚æ¥é¢„åŠ è½½æ¨¡å‹
    #     test_response = app_state.client.chat.completions.create(
    #         model=DEFAULT_MODEL,
    #         messages=[{"role": "user", "content": "ä½ å¥½"}],
    #         max_tokens=10,
    #     )
        
    #     warmup_time = time.time() - warmup_start
    #     print(f"âœ… æ¨¡å‹é¢„åŠ è½½å®Œæˆï¼Œè€—æ—¶: {warmup_time:.2f}ç§’")
    #     print(f"   æ¨¡å‹: {DEFAULT_MODEL}")
    #     print(f"   æ¨¡å‹å“åº”æµ‹è¯•: {test_response.choices[0].message.content}")
    #     app_state.model_loaded = True
        
    # except Exception as e:
    #     print(f"âŒ æ¨¡å‹é¢„åŠ è½½å¤±è´¥: {str(e)}")
    #     print("   è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ")
    #     raise
    
    # æ£€æŸ¥embeddingæ¨¡å‹
    print("\nğŸ” æ£€æŸ¥å‘é‡åµŒå…¥æ¨¡å‹...")
    try:
        test_embedding = ollama.embeddings(model='nomic-embed-text', prompt="test")
        embedding_dim = len(test_embedding["embedding"])
        print(f"âœ… å‘é‡åµŒå…¥æ¨¡å‹è¿æ¥æˆåŠŸ - æ¨¡å‹: nomic-embed-text")
        print(f"   å‘é‡ç»´åº¦: {embedding_dim}")
    except Exception as e:
        print(f"âŒ å‘é‡åµŒå…¥æ¨¡å‹è¿æ¥å¤±è´¥: {str(e)}")
        print("   è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œä¸”åŒ…å« nomic-embed-text æ¨¡å‹")
        raise
    
    # æ£€æŸ¥å‘é‡æ•°æ®åº“çŠ¶æ€
    print("\nğŸ“š æ£€æŸ¥å‘é‡æ•°æ®åº“çŠ¶æ€...")
    try:
        chunk_count = get_chunk_count()
        print(f"âœ… å‘é‡æ•°æ®åº“è¿æ¥æˆåŠŸ")
        print(f"   å½“å‰æ–‡æ¡£å—æ•°é‡: {chunk_count}")
        if chunk_count == 0:
            print("âš ï¸  è­¦å‘Š: å‘é‡æ•°æ®åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£")
    except Exception as e:
        print(f"âŒ å‘é‡æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
        raise
    
    print("\n" + "=" * 50)
    print("ğŸ‰ RAG æœåŠ¡å¯åŠ¨æˆåŠŸï¼")
    print("=" * 50)


